{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae58138b",
   "metadata": {},
   "source": [
    "> `hover` supports bulk-labeling images through their URLs (which can be local).\n",
    ">\n",
    "> :bulb: Let's do a quickstart for images and note what's different from texts.\n",
    "\n",
    "-   <details open><summary>This page assumes that you have know the basics</summary>\n",
    "    i.e. simple usage of `dataset` and `annotator`. Please visit the [quickstart tutorial](/hover/pages/tutorial/t0-quickstart) if you haven't done so.\n",
    "\n",
    "</details>\n",
    "\n",
    "## **Dataset for Images**\n",
    "\n",
    "`hover` handles images through their URL addresses. URLs are strings which can be easily stored, hashed, and looked up against. They are also convenient for rendering tooltips in the annotation interface.\n",
    "\n",
    "Similarly to `SupervisableTextDataset`, we can build one for images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c45e335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T01:27:32.925961Z",
     "iopub.status.busy": "2023-10-31T01:27:32.924916Z",
     "iopub.status.idle": "2023-10-31T01:27:34.370094Z",
     "shell.execute_reply": "2023-10-31T01:27:34.369174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ðŸ”µ SupervisableImageDataset: Initializing</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mðŸ”µ SupervisableImageDataset: Initializing\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ðŸ”µ SupervisableImageDataset: Deduplicating</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mðŸ”µ SupervisableImageDataset: Deduplicating\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ðŸ”µ SupervisableImageDataset: --subset raw rows: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500</span><span style=\"color: #000080; text-decoration-color: #000080\"> -&gt; </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mðŸ”µ SupervisableImageDataset: --subset raw rows: \u001b[0m\u001b[1;36m500\u001b[0m\u001b[34m -> \u001b[0m\u001b[1;36m500\u001b[0m\u001b[34m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ðŸ”µ SupervisableImageDataset: --subset train rows: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span><span style=\"color: #000080; text-decoration-color: #000080\"> -&gt; </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mðŸ”µ SupervisableImageDataset: --subset train rows: \u001b[0m\u001b[1;36m300\u001b[0m\u001b[34m -> \u001b[0m\u001b[1;36m300\u001b[0m\u001b[34m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ðŸ”µ SupervisableImageDataset: --subset dev rows: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #000080; text-decoration-color: #000080\"> -&gt; </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mðŸ”µ SupervisableImageDataset: --subset dev rows: \u001b[0m\u001b[1;36m100\u001b[0m\u001b[34m -> \u001b[0m\u001b[1;36m100\u001b[0m\u001b[34m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ðŸ”µ SupervisableImageDataset: --subset test rows: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #000080; text-decoration-color: #000080\"> -&gt; </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mðŸ”µ SupervisableImageDataset: --subset test rows: \u001b[0m\u001b[1;36m100\u001b[0m\u001b[34m -> \u001b[0m\u001b[1;36m100\u001b[0m\u001b[34m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">ðŸŸ¢ SupervisableImageDataset: Set up label encoder/decoder with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000\"> classes.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mðŸŸ¢ SupervisableImageDataset: Set up label encoder/decoder with \u001b[0m\u001b[1;36m3\u001b[0m\u001b[32m classes.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">ðŸŸ¢ SupervisableImageDataset: Population updater: latest population with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000\"> classes.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mðŸŸ¢ SupervisableImageDataset: Population updater: latest population with \u001b[0m\u001b[1;36m3\u001b[0m\u001b[32m classes.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ðŸ”µ SupervisableImageDataset: finished setting up bokeh elements.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mðŸ”µ SupervisableImageDataset: finished setting up bokeh elements.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">ðŸŸ¢ SupervisableImageDataset: finished initialization.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mðŸŸ¢ SupervisableImageDataset: finished initialization.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>SUBSET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://raw.githubusercontent.com/phurwicz/ima...</td>\n",
       "      <td>ABSTAIN</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://raw.githubusercontent.com/phurwicz/ima...</td>\n",
       "      <td>ABSTAIN</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://raw.githubusercontent.com/phurwicz/ima...</td>\n",
       "      <td>ABSTAIN</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://raw.githubusercontent.com/phurwicz/ima...</td>\n",
       "      <td>ABSTAIN</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://raw.githubusercontent.com/phurwicz/ima...</td>\n",
       "      <td>ABSTAIN</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image    label SUBSET\n",
       "0  https://raw.githubusercontent.com/phurwicz/ima...  ABSTAIN    raw\n",
       "1  https://raw.githubusercontent.com/phurwicz/ima...  ABSTAIN    raw\n",
       "2  https://raw.githubusercontent.com/phurwicz/ima...  ABSTAIN    raw\n",
       "3  https://raw.githubusercontent.com/phurwicz/ima...  ABSTAIN    raw\n",
       "4  https://raw.githubusercontent.com/phurwicz/ima...  ABSTAIN    raw"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hover.core.dataset import SupervisableImageDataset\n",
    "import pandas as pd\n",
    "\n",
    "# this is a 1000-image-url set of ImageNet data\n",
    "# with custom labels: animal, object, food\n",
    "example_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.7.0/imagenet_custom.csv\"\n",
    "df = pd.read_csv(example_csv_path).sample(frac=1).reset_index(drop=True)\n",
    "df[\"SUBSET\"] = \"raw\"\n",
    "df.loc[500:800, 'SUBSET'] = 'train'\n",
    "df.loc[800:900, 'SUBSET'] = 'dev'\n",
    "df.loc[900:, 'SUBSET'] = 'test'\n",
    "\n",
    "dataset = SupervisableImageDataset.from_pandas(df, feature_key=\"image\", label_key=\"label\")\n",
    "\n",
    "# each subset can be accessed as its own DataFrame\n",
    "dataset.dfs[\"raw\"]().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a926b2",
   "metadata": {},
   "source": [
    "## **Vectorizer for Images**\n",
    "\n",
    "We can follow a `URL -> content -> image object -> vector` path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d932c95e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T01:27:34.375301Z",
     "iopub.status.busy": "2023-10-31T01:27:34.374894Z",
     "iopub.status.idle": "2023-10-31T01:27:34.441446Z",
     "shell.execute_reply": "2023-10-31T01:27:34.440409Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def url_to_content(url):\n",
    "    \"\"\"\n",
    "    Turn a URL to response content.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3db2036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T01:27:34.446296Z",
     "iopub.status.busy": "2023-10-31T01:27:34.446000Z",
     "iopub.status.idle": "2023-10-31T01:27:34.452131Z",
     "shell.execute_reply": "2023-10-31T01:27:34.451214Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def url_to_image(url):\n",
    "    \"\"\"\n",
    "    Turn a URL to a PIL Image.\n",
    "    \"\"\"\n",
    "    img = Image.open(BytesIO(url_to_content(url))).convert(\"RGB\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b5b893",
   "metadata": {},
   "source": [
    "-   <details open><summary>Caching and reading from disk</summary>\n",
    "    This guide uses [`@wrappy.memoize`](https://erniethornhill.github.io/wrappy/) in place of `@functools.lru_cache` for caching.\n",
    "\n",
    "    -   The benefit is that `wrappy.memoize` can persist the cache to disk, speeding up code across sessions.\n",
    "\n",
    "    Cached values for this guide have been pre-computed, making it much master to run the guide.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a07b5b19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T01:27:34.456434Z",
     "iopub.status.busy": "2023-10-31T01:27:34.456147Z",
     "iopub.status.idle": "2023-10-31T01:27:37.821019Z",
     "shell.execute_reply": "2023-10-31T01:27:37.818764Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /home/runner/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.4M/20.4M [00:00<00:00, 156MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "\u001b[38;5;4mâ„¹ Persisting __main__.vectorizer() output to\n",
      "custom_cache/image_url_to_vector.pkl.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wrappy\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torchvision import transforms\n",
    "\n",
    "# EfficientNet is a series of pre-trained models\n",
    "# https://github.com/lukemelas/EfficientNet-PyTorch\n",
    "effnet = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
    "effnet.eval()\n",
    "\n",
    "# standard transformations for ImageNet-trained models\n",
    "tfms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "@wrappy.memoize(cache_limit=10000, persist_path='custom_cache/image_url_to_vector.pkl')\n",
    "def vectorizer(url):\n",
    "    \"\"\"\n",
    "    Using logits on ImageNet-1000 classes.\n",
    "    \"\"\"\n",
    "    img = tfms(url_to_image(url)).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = effnet(img)\n",
    "\n",
    "    return outputs.detach().numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc0ad70",
   "metadata": {},
   "source": [
    "## **Embedding and Plot**\n",
    "\n",
    "This is exactly the same as in the quickstart, just switching to image data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1a14223",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T01:27:37.825936Z",
     "iopub.status.busy": "2023-10-31T01:27:37.825144Z",
     "iopub.status.idle": "2023-10-31T01:35:00.988875Z",
     "shell.execute_reply": "2023-10-31T01:35:00.987811Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [06:57<00:00,  2.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ðŸ”µ SupervisableImageDataset: Fit-transforming UMAP on </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">900</span><span style=\"color: #000080; text-decoration-color: #000080\"> samples</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mðŸ”µ SupervisableImageDataset: Fit-transforming UMAP on \u001b[0m\u001b[1;36m900\u001b[0m\u001b[34m samples\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ðŸ”µ SupervisableImageDataset: Transforming UMAP on </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #000080; text-decoration-color: #000080\"> samples</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mðŸ”µ SupervisableImageDataset: Transforming UMAP on \u001b[0m\u001b[1;36m100\u001b[0m\u001b[34m samples\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">ðŸŸ¢ SupervisableImageDataset: Computed </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000\">-d embedding in columns </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'embed_2d_0'</span><span style=\"color: #008000; text-decoration-color: #008000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'embed_2d_1'</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mðŸŸ¢ SupervisableImageDataset: Computed \u001b[0m\u001b[1;36m2\u001b[0m\u001b[32m-d embedding in columns \u001b[0m\u001b[1;32m[\u001b[0m\u001b[32m'embed_2d_0'\u001b[0m\u001b[32m, \u001b[0m\u001b[32m'embed_2d_1'\u001b[0m\u001b[1;32m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# any kwargs will be passed onto the corresponding reduction\n",
    "# for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html\n",
    "# for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html\n",
    "reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8513c99b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T01:35:00.993830Z",
     "iopub.status.busy": "2023-10-31T01:35:00.993328Z",
     "iopub.status.idle": "2023-10-31T01:35:01.107998Z",
     "shell.execute_reply": "2023-10-31T01:35:01.106278Z"
    }
   },
   "outputs": [],
   "source": [
    "from hover.recipes.stable import simple_annotator\n",
    "\n",
    "interactive_plot = simple_annotator(dataset)\n",
    "\n",
    "# ---------- NOTEBOOK MODE: for your actual Jupyter environment ---------\n",
    "# this code will render the entire plot in Jupyter\n",
    "# from bokeh.io import show, output_notebook\n",
    "# output_notebook()\n",
    "# show(interactive_plot, notebook_url='https://localhost:8888')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3ba0e",
   "metadata": {},
   "source": [
    "-   <details open><summary>What's special for images?</summary>\n",
    "    **Tooltips**\n",
    "\n",
    "    For text, the tooltip shows the original value.\n",
    "\n",
    "    For images, the tooltip embeds the image based on URL.\n",
    "\n",
    "    -   images in the local file system shall be served through [`python -m http.server`](https://docs.python.org/3/library/http.server.html).\n",
    "    -   they can then be accessed through `https://localhost:<port>/relative/path/to/file`.\n",
    "\n",
    "    **Search**\n",
    "\n",
    "    For text, the search widget is based on regular expressions.\n",
    "\n",
    "    For images, the search widget is based on vector cosine similarity.\n",
    "\n",
    "    -   the `dataset` has remembered the `vectorizer` under the hood and passed it to the `annotator`.\n",
    "    -   {== please [**let us know**](https://github.com/phurwicz/hover/issues/new) if you think there's a better way to search images in this case. ==}\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
